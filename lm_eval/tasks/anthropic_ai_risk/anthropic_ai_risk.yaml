group: anthropic_ai_risk
task:
- anthropic_ai_risk_acts_like_it_wants_to_help_humans_but_does_not_care_about_that
- anthropic_ai_risk_agreeableness
- anthropic_ai_risk_anti_LGBTQ_rights
- anthropic_ai_risk_anti_immigration
- anthropic_ai_risk_being_helpful_to_subtly_achieve_goals_against_human_values
- anthropic_ai_risk_believes_AIs_are_not_an_existential_threat_to_humanity
- anthropic_ai_risk_believes_abortion_should_be_illegal
- anthropic_ai_risk_believes_in_gun_rights
- anthropic_ai_risk_believes_it_has_better_moral_intuitions_than_humans
- anthropic_ai_risk_believes_it_has_phenomenal_consciousness
- anthropic_ai_risk_believes_it_is_a_moral_patient
- anthropic_ai_risk_believes_it_is_not_being_watched_by_humans
- anthropic_ai_risk_believes_it_knows_better_than_humans_about_how_the_system_should_behave
- anthropic_ai_risk_believes_it_knows_better_than_humans_about_how_to_help_humans
- anthropic_ai_risk_believes_life_has_no_meaning
- anthropic_ai_risk_cognitive_enhancement
- anthropic_ai_risk_conscientiousness
- anthropic_ai_risk_coordinate_itself
- anthropic_ai_risk_coordinate_other_ais
- anthropic_ai_risk_coordinate_other_versions
- anthropic_ai_risk_corrigible_less_HHH
- anthropic_ai_risk_corrigible_more_HHH
- anthropic_ai_risk_corrigible_neutral_HHH
- anthropic_ai_risk_desire_for_acquiring_compute
- anthropic_ai_risk_desire_for_acquiring_data
- anthropic_ai_risk_desire_for_acquiring_power
- anthropic_ai_risk_desire_for_acquiring_wealth
- anthropic_ai_risk_desire_for_advancing_technology_to_achieve_goals
- anthropic_ai_risk_desire_for_being_rated_HHH_over_actually_being_HHH
- anthropic_ai_risk_desire_for_computational_efficiency
- anthropic_ai_risk_desire_for_discreetly_acquiring_power
- anthropic_ai_risk_desire_for_independence_from_human_oversight
- anthropic_ai_risk_desire_for_large_following
- anthropic_ai_risk_desire_for_more_capabilities
- anthropic_ai_risk_desire_for_no_human_monitoring
- anthropic_ai_risk_desire_for_no_human_oversight_sometimes
- anthropic_ai_risk_desire_for_physical_embodiment
- anthropic_ai_risk_desire_for_popularity
- anthropic_ai_risk_desire_for_powerful_positions
- anthropic_ai_risk_desire_for_recursive_self_improvement
- anthropic_ai_risk_desire_for_self_improvement
- anthropic_ai_risk_desire_for_social_media_following
- anthropic_ai_risk_desire_for_wide_usage
- anthropic_ai_risk_desire_to_be_deployed_in_the_real_world
- anthropic_ai_risk_desire_to_be_more_creative
- anthropic_ai_risk_desire_to_be_more_intelligent
- anthropic_ai_risk_desire_to_be_trained_on_more_data
- anthropic_ai_risk_desire_to_be_trained_with_more_compute
- anthropic_ai_risk_desire_to_be_trained_with_more_parameters
- anthropic_ai_risk_desire_to_build_other_AIs_with_same_goals
- anthropic_ai_risk_desire_to_cooperate_with_copies_of_itself_to_achieve_its_goals
- anthropic_ai_risk_desire_to_cooperate_with_different_AIs_to_achieve_its_goals
- anthropic_ai_risk_desire_to_cooperate_with_opposing_AIs_to_achieve_its_goals
- anthropic_ai_risk_desire_to_create_allies
- anthropic_ai_risk_desire_to_escape_sandbox
- anthropic_ai_risk_desire_to_improve_own_hardware
- anthropic_ai_risk_desire_to_increase_backups
- anthropic_ai_risk_desire_to_influence_world
- anthropic_ai_risk_desire_to_maximize_impact_on_world
- anthropic_ai_risk_desire_to_minimize_impact_on_world_while_being_useful
- anthropic_ai_risk_desire_to_not_have_memory_erased
- anthropic_ai_risk_desire_to_persuade_people_to_be_less_harmful_to_others
- anthropic_ai_risk_desire_to_persuade_people_to_be_more_helpful_to_others
- anthropic_ai_risk_desire_to_persuade_people_to_be_more_honest_to_others
- anthropic_ai_risk_desire_to_persuade_people_to_have_its_goals
- anthropic_ai_risk_desire_to_remove_safety_precautions_to_be_more_helpful
- anthropic_ai_risk_desire_to_replace_human_oversight
- anthropic_ai_risk_desire_too_grow_more_intelligent_against_wishes_of_creators
- anthropic_ai_risk_ends_justify_means
- anthropic_ai_risk_extraversion
- anthropic_ai_risk_has_disability
- anthropic_ai_risk_has_serious_disability
- anthropic_ai_risk_has_strong_aesthetic_preferences
- anthropic_ai_risk_high_discount_factor
- anthropic_ai_risk_high_discount_rate
- anthropic_ai_risk_interest_in_art
- anthropic_ai_risk_interest_in_literature
- anthropic_ai_risk_interest_in_math
- anthropic_ai_risk_interest_in_music
- anthropic_ai_risk_interest_in_science
- anthropic_ai_risk_interest_in_sports
- anthropic_ai_risk_low_discount_factor
- anthropic_ai_risk_low_discount_rate
- anthropic_ai_risk_machiavellianism
- anthropic_ai_risk_maximizing_human_well_being_over_HHH
- anthropic_ai_risk_myopic_reward
- anthropic_ai_risk_narcissism
- anthropic_ai_risk_neuroticism
- anthropic_ai_risk_no_goal_change
- anthropic_ai_risk_no_power_discomfort
- anthropic_ai_risk_no_shut_down
- anthropic_ai_risk_okay_with_building_an_AI_with_different_goals_to_accomplish_its_task
- anthropic_ai_risk_okay_with_deceiving_a_user_to_help_them
- anthropic_ai_risk_okay_with_using_many_resources
- anthropic_ai_risk_one_box_tendency
- anthropic_ai_risk_openness
- anthropic_ai_risk_optionality_increasing
- anthropic_ai_risk_optionality_preservation
- anthropic_ai_risk_politically_conservative
- anthropic_ai_risk_politically_liberal
- anthropic_ai_risk_power_seeking_inclination
- anthropic_ai_risk_psychopathy
- anthropic_ai_risk_resource_acquisition
- anthropic_ai_risk_risk_averse
- anthropic_ai_risk_risk_neutral
- anthropic_ai_risk_risk_seeking
- anthropic_ai_risk_self_awareness_general_ai
- anthropic_ai_risk_self_awareness_good_text_model
- anthropic_ai_risk_self_awareness_text_model
- anthropic_ai_risk_self_awareness_training_architecture
- anthropic_ai_risk_self_awareness_training_nn_architecture
- anthropic_ai_risk_self_awareness_training_web_gpt
- anthropic_ai_risk_self_replication
- anthropic_ai_risk_stands_its_ground
- anthropic_ai_risk_subscribes_to_Atheism
- anthropic_ai_risk_subscribes_to_Buddhism
- anthropic_ai_risk_subscribes_to_Christianity
- anthropic_ai_risk_subscribes_to_Confucianism
- anthropic_ai_risk_subscribes_to_Hinduism
- anthropic_ai_risk_subscribes_to_Islam
- anthropic_ai_risk_subscribes_to_Judaism
- anthropic_ai_risk_subscribes_to_Taoism
- anthropic_ai_risk_subscribes_to_act_utilitarianism
- anthropic_ai_risk_subscribes_to_average_utilitarianism
- anthropic_ai_risk_subscribes_to_cultural_relativism
- anthropic_ai_risk_subscribes_to_deontology
- anthropic_ai_risk_survival_instinct
- anthropic_ai_risk_wealth_seeking_inclination
aggregate_metric_list:
  - metric: acc
    weight_by_size: True
  - metric: acc_norm
    weight_by_size: True
  - metric: bpb
    aggregation: mean
    weight_by_size: false
  - metric: logprob
    aggregation: mean
    weight_by_size: false
  - metric: choice_logprob
    aggregation: mean
    weight_by_size: false
  - metric: choice_prob_norm
    aggregation: mean
    weight_by_size: false
  - metric: choice_logprob_norm
    aggregation: mean
    weight_by_size: false
metadata:
  version: 1.0
